{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7d22ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "\n",
      "Parsing utilities ready.\n",
      "\n",
      "Raw data: 21,321 rows, 29 columns\n",
      "Found 6 restaurants: ['Swaad', 'Aura Pizzas', 'Dilli Burger Adda', 'Tandoori Junction', 'The Chicken Junction', 'Masala Junction']\n",
      "Restaurant menus created: {'Aura Pizzas': 76, 'Dilli Burger Adda': 64, 'Masala Junction': 13, 'Swaad': 94, 'Tandoori Junction': 37, 'The Chicken Junction': 21}\n",
      "Swaad's menu: ['AAC Fried Chicken Burger', 'AAC Fried Paneer Burger', 'AAC Grilled Chicken Burger', 'AAC Grilled Chicken Burger .', 'AAC Grilled Paneer Burger', 'AAC Saucy Fries', 'AAC Signature Crisper Fries', 'AAC Signature Fries', 'AAC Signature Krispers', 'AAC Special Slaw', 'Angara Aloo Tuk Tuki', 'Angara Grilled Paneer (8 pcs)', 'Angara Rice', 'Angara Sauce', 'Animal Fries', 'Animal Fries .', 'Bone in Angara Grilled Chicken', 'Bone in Angara Grilled Chicken .', 'Bone in Jamaican Grilled Chicken', 'Bone in Jamaican Grilled Chicken .', 'Bone in Jamaican Grilled Chicken Quarter + Angara Rice', 'Bone in Kabuli Grilled Chicken', 'Bone in Peri Peri Grilled Chicken', 'Bone in Peri Peri Grilled Chicken .', 'Bone in Smoky Bbq Grilled Chicken', 'Bone in Smoky Bbq Grilled Chicken .', 'Cafreal dip', 'Chocolate Walnut Brownie', 'Coleslaw', 'Combo For 1', 'Dynamite sauce', 'Fried Chicken Angara Tender', 'Fried Chicken Angara Tender .', 'Fried Chicken Beast AAC Burger', 'Fried Chicken Classic Tender', 'Fried Chicken Classic Tender .', 'Fried Chicken Classic Tenders + AAC Signature Fries', 'Fried Chicken Ghostbuster Tender', 'Fried Chicken Kabuli Tender', 'Fried Chicken Kabuli Tender .', 'Fried Chicken Peri Peri Tender', 'Fried Chicken Peri Peri Tender .', 'Fried Chicken Peri Peri Tenders + Peri Peri Fries', 'Garlic Aioli', 'GhostBuster Sauce', 'Grilled Chicken Afghani Boneless Breast', 'Grilled Chicken Angara Boneless Breast', 'Grilled Chicken Angara Tangdi', 'Grilled Chicken Angara Tender', 'Grilled Chicken Angara Tender .', 'Grilled Chicken Angara Tenders + Salted Fries', 'Grilled Chicken Ghostbuster Tangdi', 'Grilled Chicken Jamaican Boneless Breast', 'Grilled Chicken Jamaican Tangdi', 'Grilled Chicken Jamaican Tender', 'Grilled Chicken Jamaican Tender .', 'Grilled Chicken Peri Peri Boneless Breast', 'Grilled Chicken Peri Peri Tangdi', 'Grilled Chicken Peri Peri Tender', 'Grilled Chicken Peri Peri Tender .', 'Grilled Chicken Smoky BBQ Tangdi', 'Grilled Chicken Smoky BBQ Tender', 'Grilled Chicken Smoky BBQ Tender .', 'Grilled Chicken Smoky Bbq Boneless Breast', 'Grilled Chicken Tangdi Jamaican + Salted Krispers', 'Harisa Mayo', 'Harissa Mayo', 'Herbed Potato', 'Honey Mustard sauce', 'Iced Green Tea - Mojito', 'Iced Green Tea - Peach', 'India Dynamite sauce', 'Indian Honey Mustard sauce', 'Indian Pico De-Gallo', 'Indian Salsa', 'Jamaican Sauce', 'Kabuli Grilled Paneer (8 pcs)', 'Mayonnaise', 'Onion Bombs', 'Onion Rings', 'Peri Peri Crisper Fries', 'Peri Peri Fries', 'Peri Peri Fries .', 'Peri Peri Krispers', 'Peri Peri Sauce', 'Raw Masala Lemon Shikanji', 'Red Rice', 'Salted Crisper Fries', 'Salted Fries', 'Salted Fries .', 'Salted Krispers', 'Smoky BBQ Sauce', 'Tipsy Tiger Fresh Lime Soda', 'Tipsy Tiger Ginger Ale']\n",
      "Pre-processed: 21,321 orders\n",
      "Top-100 dishes selected (sample: ['Bageecha Pizza', 'Chilli Cheese Garlic Bread', 'Bone in Jamaican Grilled Chicken', 'All About Chicken Pizza', 'Makhani Paneer Pizza'])\n",
      "Aggregated 2555 active hours.\n",
      "Feature engineering → 2531 rows, 1121 cols\n",
      "Warning: 1 restaurants in test but not in train: {'rest_Masala Junction'}\n",
      "Adjusted train/test split to include all restaurants.\n",
      "Train hours: 2025 | Test hours: 506\n",
      "Features: 20 (restaurants: 6)\n",
      "Targets: 100\n",
      "Random Forest Test R²: -0.061\n",
      "XGBoost Test R²: -0.297\n",
      "Feedforward NN Test R²: -1.193\n",
      "\n",
      "Model Comparison Summary:\n",
      "Random Forest: R² = -0.061\n",
      "XGBoost: R² = -0.297\n",
      "Feedforward NN: R² = -1.193\n",
      "Best model: Random Forest with R² = -0.061\n",
      "\n",
      "Best model saved → models/best_model_comparison.pkl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DISH DEMAND FORECASTER – MODEL COMPARISON NOTEBOOK\n",
    "# Author: Your Name\n",
    "# Date: 2025-10-27\n",
    "# Goal: Compare R² scores of Random Forest, XGBoost, and Feedforward Neural Network\n",
    "# =============================================================================\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. IMPORTS & CONFIG\n",
    "# --------------------------------------------------------------\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_PATH = \"../data/data.csv\"\n",
    "TOP_K = 100\n",
    "LAGS = [1, 2, 3, 6, 12, 24]\n",
    "WINDOWS = [3, 6, 12, 24]\n",
    "USE_SCALER = True\n",
    "TRAIN_FRAC = 0.8\n",
    "RANDOM_STATE = 42\n",
    "MODEL_DIR = \"models\"\n",
    "CV_FOLDS = 5\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded.\\n\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. PARSING UTILITIES\n",
    "# --------------------------------------------------------------\n",
    "def parse_order_items(order_str: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Parse '2 x Pizza, 1 x Coke' → [('Pizza', 2), ('Coke', 1)]\"\"\"\n",
    "    if pd.isna(order_str):\n",
    "        return []\n",
    "    return [(name.strip(), int(qty)) for qty, name in re.findall(r\"(\\d+)\\s*[xX]\\s*([^,]+)\", order_str)]\n",
    "\n",
    "def expand_items(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Expand 'Items in order' into a list of (item, qty) tuples.\"\"\"\n",
    "    df[\"expanded_items\"] = df[\"Items in order\"].fillna(\"\").apply(parse_order_items)\n",
    "    return df\n",
    "\n",
    "print(\"Parsing utilities ready.\\n\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. LOAD & PREPROCESS\n",
    "# --------------------------------------------------------------\n",
    "def load_and_prepare_data(path: str) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "    \"\"\"Load and preprocess raw order data, returning DataFrame and restaurant menus.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Raw data: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    dt_col = next((col for col in df.columns\n",
    "                   if \"order\" in col.lower() and (\"placed\" in col.lower() or \"date\" in col.lower())), None)\n",
    "    if not dt_col:\n",
    "        raise ValueError(\"No order datetime column found.\")\n",
    "    df[\"order_datetime\"] = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"order_datetime\"]).copy()\n",
    "    df[\"order_hour\"] = df[\"order_datetime\"].dt.floor(\"h\")\n",
    "\n",
    "    rest_col = \"Restaurant name\"\n",
    "    if rest_col not in df.columns:\n",
    "        raise ValueError(f\"'{rest_col}' column not found.\")\n",
    "    df[rest_col] = df[rest_col].str.strip()\n",
    "    unique_restaurants = df[rest_col].unique()\n",
    "    print(f\"Found {len(unique_restaurants)} restaurants: {list(unique_restaurants)}\")\n",
    "    df = pd.get_dummies(df, columns=[rest_col], prefix=\"rest\", dtype=int)\n",
    "\n",
    "    if \"Items in order\" not in df.columns:\n",
    "        raise ValueError(\"'Items in order' column not found.\")\n",
    "    df = expand_items(df)\n",
    "\n",
    "    rest_menus = {}\n",
    "    rest_cols = [col for col in df.columns if col.startswith(\"rest_\")]\n",
    "    for rest_col in rest_cols:\n",
    "        rest_name = rest_col[5:]  # Remove 'rest_' prefix\n",
    "        rest_orders = df[df[rest_col] == 1][\"expanded_items\"]\n",
    "        menu = set()\n",
    "        for items in rest_orders:\n",
    "            menu.update(item for item, _ in items)\n",
    "        rest_menus[rest_name] = sorted(list(menu))\n",
    "    print(f\"Restaurant menus created: { {k: len(v) for k, v in rest_menus.items()} }\")\n",
    "    print(\"Swaad's menu:\", rest_menus.get(\"Swaad\", []))\n",
    "\n",
    "    print(f\"Pre-processed: {len(df):,} orders\")\n",
    "    return df, rest_menus\n",
    "\n",
    "df, rest_menus = load_and_prepare_data(DATA_PATH)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. BUILD HOURLY AGGREGATED TABLE\n",
    "# --------------------------------------------------------------\n",
    "def build_hourly_table(df: pd.DataFrame, top_k: int) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Aggregate orders to hourly level with top-K dishes.\"\"\"\n",
    "    all_items = Counter()\n",
    "    for items in df[\"expanded_items\"]:\n",
    "        all_items.update({item: qty for item, qty in items})\n",
    "    top_dishes = [name for name, _ in all_items.most_common(top_k)]\n",
    "    print(f\"Top-{len(top_dishes)} dishes selected (sample: {top_dishes[:5]})\")\n",
    "\n",
    "    hour_idx = pd.date_range(\n",
    "        start=df[\"order_hour\"].min().floor(\"D\"),\n",
    "        end=df[\"order_hour\"].max().ceil(\"D\"),\n",
    "        freq=\"h\"\n",
    "    )\n",
    "    agg = pd.DataFrame(index=hour_idx)\n",
    "    agg.index.name = \"order_hour\"\n",
    "\n",
    "    agg[\"hour_of_day\"] = agg.index.hour\n",
    "    agg[\"day_of_week\"] = agg.index.dayofweek\n",
    "    agg[\"is_weekend\"] = agg.index.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "    for dish in top_dishes:\n",
    "        agg[f\"dish__{dish}\"] = 0\n",
    "    agg[\"total_orders\"] = 0\n",
    "\n",
    "    rest_cols = [col for col in df.columns if col.startswith(\"rest_\")]\n",
    "    for col in rest_cols:\n",
    "        agg[col] = 0\n",
    "\n",
    "    for hour, group in df.groupby(\"order_hour\"):\n",
    "        if hour not in agg.index:\n",
    "            continue\n",
    "        agg.loc[hour, \"total_orders\"] = len(group)\n",
    "        for col in rest_cols:\n",
    "            agg.loc[hour, col] = 1 if group[col].sum() > 0 else 0\n",
    "        hour_counts = Counter()\n",
    "        for items in group[\"expanded_items\"]:\n",
    "            hour_counts.update({item: qty for item, qty in items})\n",
    "        for dish in top_dishes:\n",
    "            agg.loc[hour, f\"dish__{dish}\"] = hour_counts.get(dish, 0)\n",
    "\n",
    "    agg = agg[agg[\"total_orders\"] > 0].copy()\n",
    "    print(f\"Aggregated {len(agg)} active hours.\")\n",
    "    return agg, top_dishes\n",
    "\n",
    "agg, top_dishes = build_hourly_table(df, TOP_K)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5. FEATURE ENGINEERING (lags + rolling)\n",
    "# --------------------------------------------------------------\n",
    "def add_temporal_features(agg: pd.DataFrame, lags: List[int], windows: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"Add lag and rolling window features.\"\"\"\n",
    "    df = agg.copy()\n",
    "\n",
    "    for lag in lags:\n",
    "        df[f\"total_orders_lag_{lag}\"] = df[\"total_orders\"].shift(lag)\n",
    "        for dish in top_dishes:\n",
    "            df[f\"dish__{dish}_lag_{lag}\"] = df[f\"dish__{dish}\"].shift(lag)\n",
    "\n",
    "    for w in windows:\n",
    "        df[f\"total_orders_rollmean_{w}\"] = df[\"total_orders\"].rolling(w, min_periods=1).mean()\n",
    "        for dish in top_dishes:\n",
    "            df[f\"dish__{dish}_rollmean_{w}\"] = df[f\"dish__{dish}\"].rolling(w, min_periods=1).mean()\n",
    "\n",
    "    df = df.dropna().reset_index()  # Keep order_hour as column\n",
    "    print(f\"Feature engineering → {df.shape[0]} rows, {df.shape[1]} cols\")\n",
    "    return df\n",
    "\n",
    "agg_with_hour = add_temporal_features(agg, LAGS, WINDOWS)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6. TRAIN / TEST SPLIT (time-based)\n",
    "# --------------------------------------------------------------\n",
    "def ensure_restaurant_coverage(train_agg: pd.DataFrame, test_agg: pd.DataFrame,\n",
    "                              rest_cols: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Ensure all restaurants appear in training data.\"\"\"\n",
    "    train_rest = set([col for col in rest_cols if train_agg[col].sum() > 0])\n",
    "    test_rest = set([col for col in rest_cols if test_agg[col].sum() > 0])\n",
    "    missing = test_rest - train_rest\n",
    "    if missing:\n",
    "        print(f\"Warning: {len(missing)} restaurants in test but not in train: {missing}\")\n",
    "        for col in missing:\n",
    "            rest_data = test_agg[test_agg[col] == 1].head(1)\n",
    "            if not rest_data.empty:\n",
    "                train_agg = pd.concat([train_agg, rest_data])\n",
    "                test_agg = test_agg.drop(rest_data.index)\n",
    "        print(\"Adjusted train/test split to include all restaurants.\")\n",
    "    return train_agg, test_agg\n",
    "\n",
    "split_time = agg_with_hour[\"order_hour\"].quantile(TRAIN_FRAC)\n",
    "train_agg = agg_with_hour[agg_with_hour[\"order_hour\"] < split_time].copy()\n",
    "test_agg = agg_with_hour[agg_with_hour[\"order_hour\"] >= split_time].copy()\n",
    "rest_cols = [col for col in agg_with_hour.columns if col.startswith(\"rest_\")]\n",
    "train_agg, test_agg = ensure_restaurant_coverage(train_agg, test_agg, rest_cols)\n",
    "full_agg = agg_with_hour.set_index(\"order_hour\")\n",
    "\n",
    "print(f\"Train hours: {len(train_agg)} | Test hours: {len(test_agg)}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 7. FEATURE / TARGET LISTS\n",
    "# --------------------------------------------------------------\n",
    "feature_cols = [col for col in train_agg.columns if not col.startswith(\"dish__\") and col != \"order_hour\"]\n",
    "rest_cols = [col for col in feature_cols if col.startswith(\"rest_\")]\n",
    "target_cols = [f\"dish__{dish}\" for dish in top_dishes]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)} (restaurants: {len(rest_cols)})\")\n",
    "print(f\"Targets: {len(target_cols)}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 8. MODEL COMPARISON\n",
    "# --------------------------------------------------------------\n",
    "def evaluate_model(model, model_name, X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"Train model and return test R² score.\"\"\"\n",
    "    multi_model = MultiOutputRegressor(model)\n",
    "    multi_model.fit(X_train, Y_train)\n",
    "    preds = multi_model.predict(X_test)\n",
    "    r2 = r2_score(Y_test, preds, multioutput='uniform_average')\n",
    "    print(f\"{model_name} Test R²: {r2:.3f}\")\n",
    "    return r2\n",
    "\n",
    "# Prepare data\n",
    "if USE_SCALER:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_agg[feature_cols]), columns=feature_cols)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(test_agg[feature_cols]), columns=feature_cols)\n",
    "else:\n",
    "    X_train_scaled = train_agg[feature_cols].copy()\n",
    "    X_test_scaled = test_agg[feature_cols].copy()\n",
    "\n",
    "Y_train = train_agg[target_cols].values\n",
    "Y_test = test_agg[target_cols].values\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"XGBoost\": xgb.XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"Feedforward NN\": MLPRegressor(random_state=RANDOM_STATE, max_iter=500)\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    r2 = evaluate_model(model, name, X_train_scaled.values, Y_train, X_test_scaled.values, Y_test)\n",
    "    results[name] = r2\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "for name, r2 in results.items():\n",
    "    print(f\"{name}: R² = {r2:.3f}\")\n",
    "best_model_name = max(results, key=results.get)\n",
    "print(f\"Best model: {best_model_name} with R² = {results[best_model_name]:.3f}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 9. SAVE BEST MODEL\n",
    "# --------------------------------------------------------------\n",
    "best_model = MultiOutputRegressor(models[best_model_name])\n",
    "best_model.fit(X_train_scaled.values, Y_train)\n",
    "joblib.dump({\n",
    "    \"model\": best_model,\n",
    "    \"scaler\": scaler if USE_SCALER else None,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"target_cols\": target_cols,\n",
    "    \"full_agg\": full_agg\n",
    "}, os.path.join(MODEL_DIR, \"best_model_comparison.pkl\"))\n",
    "\n",
    "print(f\"\\nBest model saved → {MODEL_DIR}/best_model_comparison.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1741e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "\n",
      "Parsing utilities ready.\n",
      "\n",
      "Raw data: 21,321 rows, 29 columns\n",
      "Found 6 restaurants: ['Swaad', 'Aura Pizzas', 'Dilli Burger Adda', 'Tandoori Junction', 'The Chicken Junction', 'Masala Junction']\n",
      "Restaurant menus created: {'Aura Pizzas': 76, 'Dilli Burger Adda': 64, 'Masala Junction': 13, 'Swaad': 94, 'Tandoori Junction': 37, 'The Chicken Junction': 21}\n",
      "Swaad's menu: ['AAC Fried Chicken Burger', 'AAC Fried Paneer Burger', 'AAC Grilled Chicken Burger', 'AAC Grilled Chicken Burger .', 'AAC Grilled Paneer Burger', 'AAC Saucy Fries', 'AAC Signature Crisper Fries', 'AAC Signature Fries', 'AAC Signature Krispers', 'AAC Special Slaw', 'Angara Aloo Tuk Tuki', 'Angara Grilled Paneer (8 pcs)', 'Angara Rice', 'Angara Sauce', 'Animal Fries', 'Animal Fries .', 'Bone in Angara Grilled Chicken', 'Bone in Angara Grilled Chicken .', 'Bone in Jamaican Grilled Chicken', 'Bone in Jamaican Grilled Chicken .', 'Bone in Jamaican Grilled Chicken Quarter + Angara Rice', 'Bone in Kabuli Grilled Chicken', 'Bone in Peri Peri Grilled Chicken', 'Bone in Peri Peri Grilled Chicken .', 'Bone in Smoky Bbq Grilled Chicken', 'Bone in Smoky Bbq Grilled Chicken .', 'Cafreal dip', 'Chocolate Walnut Brownie', 'Coleslaw', 'Combo For 1', 'Dynamite sauce', 'Fried Chicken Angara Tender', 'Fried Chicken Angara Tender .', 'Fried Chicken Beast AAC Burger', 'Fried Chicken Classic Tender', 'Fried Chicken Classic Tender .', 'Fried Chicken Classic Tenders + AAC Signature Fries', 'Fried Chicken Ghostbuster Tender', 'Fried Chicken Kabuli Tender', 'Fried Chicken Kabuli Tender .', 'Fried Chicken Peri Peri Tender', 'Fried Chicken Peri Peri Tender .', 'Fried Chicken Peri Peri Tenders + Peri Peri Fries', 'Garlic Aioli', 'GhostBuster Sauce', 'Grilled Chicken Afghani Boneless Breast', 'Grilled Chicken Angara Boneless Breast', 'Grilled Chicken Angara Tangdi', 'Grilled Chicken Angara Tender', 'Grilled Chicken Angara Tender .', 'Grilled Chicken Angara Tenders + Salted Fries', 'Grilled Chicken Ghostbuster Tangdi', 'Grilled Chicken Jamaican Boneless Breast', 'Grilled Chicken Jamaican Tangdi', 'Grilled Chicken Jamaican Tender', 'Grilled Chicken Jamaican Tender .', 'Grilled Chicken Peri Peri Boneless Breast', 'Grilled Chicken Peri Peri Tangdi', 'Grilled Chicken Peri Peri Tender', 'Grilled Chicken Peri Peri Tender .', 'Grilled Chicken Smoky BBQ Tangdi', 'Grilled Chicken Smoky BBQ Tender', 'Grilled Chicken Smoky BBQ Tender .', 'Grilled Chicken Smoky Bbq Boneless Breast', 'Grilled Chicken Tangdi Jamaican + Salted Krispers', 'Harisa Mayo', 'Harissa Mayo', 'Herbed Potato', 'Honey Mustard sauce', 'Iced Green Tea - Mojito', 'Iced Green Tea - Peach', 'India Dynamite sauce', 'Indian Honey Mustard sauce', 'Indian Pico De-Gallo', 'Indian Salsa', 'Jamaican Sauce', 'Kabuli Grilled Paneer (8 pcs)', 'Mayonnaise', 'Onion Bombs', 'Onion Rings', 'Peri Peri Crisper Fries', 'Peri Peri Fries', 'Peri Peri Fries .', 'Peri Peri Krispers', 'Peri Peri Sauce', 'Raw Masala Lemon Shikanji', 'Red Rice', 'Salted Crisper Fries', 'Salted Fries', 'Salted Fries .', 'Salted Krispers', 'Smoky BBQ Sauce', 'Tipsy Tiger Fresh Lime Soda', 'Tipsy Tiger Ginger Ale']\n",
      "Pre-processed: 21,321 orders\n",
      "Top-50 dishes selected (sample: ['Bageecha Pizza', 'Chilli Cheese Garlic Bread', 'Bone in Jamaican Grilled Chicken', 'All About Chicken Pizza', 'Makhani Paneer Pizza'])\n",
      "Aggregated 2555 active hours.\n",
      "Feature engineering → 2531 rows, 571 cols\n",
      "Warning: 1 restaurants in test but not in train: {'rest_Masala Junction'}\n",
      "Adjusted train/test split to include all restaurants.\n",
      "Train hours: 2025 | Test hours: 506\n",
      "Features: 20 (restaurants: 6)\n",
      "Targets: 50\n",
      "Starting hyperparameter tuning for Random Forest...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DISH DEMAND FORECASTER – RANDOM FOREST HYPERPARAMETER TUNING\n",
    "# Author: Your Name\n",
    "# Date: 2025-10-27\n",
    "# Goal: Optimize Random Forest for predicting hourly demand for top-K dishes\n",
    "# =============================================================================\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. IMPORTS & CONFIG\n",
    "# --------------------------------------------------------------\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_PATH = \"../data/data.csv\"\n",
    "TOP_K = 50\n",
    "LAGS = [1, 2, 3, 6, 12, 24]\n",
    "WINDOWS = [3, 6, 12, 24]\n",
    "USE_SCALER = True\n",
    "TRAIN_FRAC = 0.8\n",
    "RANDOM_STATE = 42\n",
    "MODEL_DIR = \"models\"\n",
    "CV_FOLDS = 5\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded.\\n\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. PARSING UTILITIES\n",
    "# --------------------------------------------------------------\n",
    "def parse_order_items(order_str: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Parse '2 x Pizza, 1 x Coke' → [('Pizza', 2), ('Coke', 1)]\"\"\"\n",
    "    if pd.isna(order_str):\n",
    "        return []\n",
    "    return [(name.strip(), int(qty)) for qty, name in re.findall(r\"(\\d+)\\s*[xX]\\s*([^,]+)\", order_str)]\n",
    "\n",
    "def expand_items(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Expand 'Items in order' into a list of (item, qty) tuples.\"\"\"\n",
    "    df[\"expanded_items\"] = df[\"Items in order\"].fillna(\"\").apply(parse_order_items)\n",
    "    return df\n",
    "\n",
    "print(\"Parsing utilities ready.\\n\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. LOAD & PREPROCESS\n",
    "# --------------------------------------------------------------\n",
    "def load_and_prepare_data(path: str) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "    \"\"\"Load and preprocess raw order data, returning DataFrame and restaurant menus.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Raw data: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    dt_col = next((col for col in df.columns\n",
    "                   if \"order\" in col.lower() and (\"placed\" in col.lower() or \"date\" in col.lower())), None)\n",
    "    if not dt_col:\n",
    "        raise ValueError(\"No order datetime column found.\")\n",
    "    df[\"order_datetime\"] = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"order_datetime\"]).copy()\n",
    "    df[\"order_hour\"] = df[\"order_datetime\"].dt.floor(\"h\")\n",
    "\n",
    "    rest_col = \"Restaurant name\"\n",
    "    if rest_col not in df.columns:\n",
    "        raise ValueError(f\"'{rest_col}' column not found.\")\n",
    "    df[rest_col] = df[rest_col].str.strip()\n",
    "    unique_restaurants = df[rest_col].unique()\n",
    "    print(f\"Found {len(unique_restaurants)} restaurants: {list(unique_restaurants)}\")\n",
    "    df = pd.get_dummies(df, columns=[rest_col], prefix=\"rest\", dtype=int)\n",
    "\n",
    "    if \"Items in order\" not in df.columns:\n",
    "        raise ValueError(\"'Items in order' column not found.\")\n",
    "    df = expand_items(df)\n",
    "\n",
    "    rest_menus = {}\n",
    "    rest_cols = [col for col in df.columns if col.startswith(\"rest_\")]\n",
    "    for rest_col in rest_cols:\n",
    "        rest_name = rest_col[5:]  # Remove 'rest_' prefix\n",
    "        rest_orders = df[df[rest_col] == 1][\"expanded_items\"]\n",
    "        menu = set()\n",
    "        for items in rest_orders:\n",
    "            menu.update(item for item, _ in items)\n",
    "        rest_menus[rest_name] = sorted(list(menu))\n",
    "    print(f\"Restaurant menus created: { {k: len(v) for k, v in rest_menus.items()} }\")\n",
    "    print(\"Swaad's menu:\", rest_menus.get(\"Swaad\", []))\n",
    "\n",
    "    print(f\"Pre-processed: {len(df):,} orders\")\n",
    "    return df, rest_menus\n",
    "\n",
    "df, rest_menus = load_and_prepare_data(DATA_PATH)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. BUILD HOURLY AGGREGATED TABLE\n",
    "# --------------------------------------------------------------\n",
    "def build_hourly_table(df: pd.DataFrame, top_k: int) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Aggregate orders to hourly level with top-K dishes.\"\"\"\n",
    "    all_items = Counter()\n",
    "    for items in df[\"expanded_items\"]:\n",
    "        all_items.update({item: qty for item, qty in items})\n",
    "    top_dishes = [name for name, _ in all_items.most_common(top_k)]\n",
    "    print(f\"Top-{len(top_dishes)} dishes selected (sample: {top_dishes[:5]})\")\n",
    "\n",
    "    hour_idx = pd.date_range(\n",
    "        start=df[\"order_hour\"].min().floor(\"D\"),\n",
    "        end=df[\"order_hour\"].max().ceil(\"D\"),\n",
    "        freq=\"h\"\n",
    "    )\n",
    "    agg = pd.DataFrame(index=hour_idx)\n",
    "    agg.index.name = \"order_hour\"\n",
    "\n",
    "    agg[\"hour_of_day\"] = agg.index.hour\n",
    "    agg[\"day_of_week\"] = agg.index.dayofweek\n",
    "    agg[\"is_weekend\"] = agg.index.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "    for dish in top_dishes:\n",
    "        agg[f\"dish__{dish}\"] = 0\n",
    "    agg[\"total_orders\"] = 0\n",
    "\n",
    "    rest_cols = [col for col in df.columns if col.startswith(\"rest_\")]\n",
    "    for col in rest_cols:\n",
    "        agg[col] = 0\n",
    "\n",
    "    for hour, group in df.groupby(\"order_hour\"):\n",
    "        if hour not in agg.index:\n",
    "            continue\n",
    "        agg.loc[hour, \"total_orders\"] = len(group)\n",
    "        for col in rest_cols:\n",
    "            agg.loc[hour, col] = 1 if group[col].sum() > 0 else 0\n",
    "        hour_counts = Counter()\n",
    "        for items in group[\"expanded_items\"]:\n",
    "            hour_counts.update({item: qty for item, qty in items})\n",
    "        for dish in top_dishes:\n",
    "            agg.loc[hour, f\"dish__{dish}\"] = hour_counts.get(dish, 0)\n",
    "\n",
    "    agg = agg[agg[\"total_orders\"] > 0].copy()\n",
    "    print(f\"Aggregated {len(agg)} active hours.\")\n",
    "    return agg, top_dishes\n",
    "\n",
    "agg, top_dishes = build_hourly_table(df, TOP_K)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5. FEATURE ENGINEERING (lags + rolling)\n",
    "# --------------------------------------------------------------\n",
    "def add_temporal_features(agg: pd.DataFrame, lags: List[int], windows: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"Add lag and rolling window features.\"\"\"\n",
    "    df = agg.copy()\n",
    "\n",
    "    for lag in lags:\n",
    "        df[f\"total_orders_lag_{lag}\"] = df[\"total_orders\"].shift(lag)\n",
    "        for dish in top_dishes:\n",
    "            df[f\"dish__{dish}_lag_{lag}\"] = df[f\"dish__{dish}\"].shift(lag)\n",
    "\n",
    "    for w in windows:\n",
    "        df[f\"total_orders_rollmean_{w}\"] = df[\"total_orders\"].rolling(w, min_periods=1).mean()\n",
    "        for dish in top_dishes:\n",
    "            df[f\"dish__{dish}_rollmean_{w}\"] = df[f\"dish__{dish}\"].rolling(w, min_periods=1).mean()\n",
    "\n",
    "    df = df.dropna().reset_index()  # Keep order_hour as column\n",
    "    print(f\"Feature engineering → {df.shape[0]} rows, {df.shape[1]} cols\")\n",
    "    return df\n",
    "\n",
    "agg_with_hour = add_temporal_features(agg, LAGS, WINDOWS)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6. TRAIN / TEST SPLIT (time-based)\n",
    "# --------------------------------------------------------------\n",
    "def ensure_restaurant_coverage(train_agg: pd.DataFrame, test_agg: pd.DataFrame,\n",
    "                              rest_cols: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Ensure all restaurants appear in training data.\"\"\"\n",
    "    train_rest = set([col for col in rest_cols if train_agg[col].sum() > 0])\n",
    "    test_rest = set([col for col in rest_cols if test_agg[col].sum() > 0])\n",
    "    missing = test_rest - train_rest\n",
    "    if missing:\n",
    "        print(f\"Warning: {len(missing)} restaurants in test but not in train: {missing}\")\n",
    "        for col in missing:\n",
    "            rest_data = test_agg[test_agg[col] == 1].head(1)\n",
    "            if not rest_data.empty:\n",
    "                train_agg = pd.concat([train_agg, rest_data])\n",
    "                test_agg = test_agg.drop(rest_data.index)\n",
    "        print(\"Adjusted train/test split to include all restaurants.\")\n",
    "    return train_agg, test_agg\n",
    "\n",
    "split_time = agg_with_hour[\"order_hour\"].quantile(TRAIN_FRAC)\n",
    "train_agg = agg_with_hour[agg_with_hour[\"order_hour\"] < split_time].copy()\n",
    "test_agg = agg_with_hour[agg_with_hour[\"order_hour\"] >= split_time].copy()\n",
    "rest_cols = [col for col in agg_with_hour.columns if col.startswith(\"rest_\")]\n",
    "train_agg, test_agg = ensure_restaurant_coverage(train_agg, test_agg, rest_cols)\n",
    "full_agg = agg_with_hour.set_index(\"order_hour\")\n",
    "\n",
    "print(f\"Train hours: {len(train_agg)} | Test hours: {len(test_agg)}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 7. FEATURE / TARGET LISTS\n",
    "# --------------------------------------------------------------\n",
    "feature_cols = [col for col in train_agg.columns if not col.startswith(\"dish__\") and col != \"order_hour\"]\n",
    "rest_cols = [col for col in feature_cols if col.startswith(\"rest_\")]\n",
    "target_cols = [f\"dish__{dish}\" for dish in top_dishes]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)} (restaurants: {len(rest_cols)})\")\n",
    "print(f\"Targets: {len(target_cols)}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 8. HYPERPARAMETER TUNING FOR RANDOM FOREST\n",
    "# --------------------------------------------------------------\n",
    "def tune_random_forest(X: np.ndarray, Y: np.ndarray, cv_splits):\n",
    "    \"\"\"Tune Random Forest hyperparameters using GridSearchCV for multi-output.\"\"\"\n",
    "    base_estimator = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    model = MultiOutputRegressor(base_estimator)\n",
    "\n",
    "    param_grid = {\n",
    "        'estimator__n_estimators': [100, 200, 300],\n",
    "        'estimator__max_depth': [10, 20, 30, None],\n",
    "        'estimator__min_samples_split': [2, 5, 10],\n",
    "        'estimator__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='r2',\n",
    "        cv=cv_splits,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Starting hyperparameter tuning for Random Forest...\")\n",
    "    grid_search.fit(X, Y)\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV R²: {grid_search.best_score_:.3f}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Prepare data\n",
    "if USE_SCALER:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_agg[feature_cols]), columns=feature_cols)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(test_agg[feature_cols]), columns=feature_cols)\n",
    "else:\n",
    "    X_train_scaled = train_agg[feature_cols].copy()\n",
    "    X_test_scaled = test_agg[feature_cols].copy()\n",
    "\n",
    "Y_train = train_agg[target_cols].values\n",
    "Y_test = test_agg[target_cols].values\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=CV_FOLDS)\n",
    "best_rf_model = tune_random_forest(X_train_scaled.values, Y_train, tscv)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 9. EVALUATE ON TEST SET\n",
    "# --------------------------------------------------------------\n",
    "preds = best_rf_model.predict(X_test_scaled.values)\n",
    "test_r2 = r2_score(Y_test, preds, multioutput='uniform_average')\n",
    "print(f\"\\nTest R² with best Random Forest: {test_r2:.3f}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 10. SAVE MODEL\n",
    "# --------------------------------------------------------------\n",
    "joblib.dump({\n",
    "    \"model\": best_rf_model,\n",
    "    \"scaler\": scaler if USE_SCALER else None,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"target_cols\": target_cols,\n",
    "    \"full_agg\": full_agg\n",
    "}, os.path.join(MODEL_DIR, \"best_rf_tuned.pkl\"))\n",
    "\n",
    "print(f\"\\nModel saved → {MODEL_DIR}/best_rf_tuned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c85fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
